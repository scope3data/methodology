---
title: "Inference Service"
description: "Methodology for calculating the impact of managing an AI inference service"
---

## Overview

An "inference service" is a service that exposes a single model for inference. This service will be hosted on a cluster of servers or cloud instances, potentially with reserved capacity and the ability to auto-scale with volume.

Model:
- The base model used including the current [amortized training cost per inference](/training#amortization-of-impact-across-use-life)
- The current [amortized fine-tuning cost per inference](/fine_tuning#amortization-of-fine-tuning-impact-across-use-life) if applicable

Cluster data:
- [Cluster](/cluster) details
- Cluster location(s) - if hosted in a cloud, which region(s)
- Cluster throughput (per server/instance if autoscaling)

## Activity data

The activity of the inference service should be provided on an hourly basis to map to grid intensity from renewables:
- Date and time, aggregated by hour
- Cluster size (as multiple of base cluster)
- Average CPU and GPU utilization
- For key parameters (image size & steps, input/output tokens):
  - Number of requests
  - Request latency (or other proxy for computation)

## Calculating emissions and water usage

For a given hour:
```
usage emissions = (cluster size) x E(gpu%, cpu%) x (datacenter PUE) x (average grid intensity)

embodied emissions = (cluster size) x EmbEm(1)

total emissions = (usage emissions) + (embodied emissions)

water usage = (cluster size) x E(gpu%, cpu%) x (cluster WUE)

embodied water usage = (cluster size) x EmbH2O(1)

total water = (water usage) + (embodied water usage)
```

For a set of parameters, calculate the intensity as the proportion of total computation:
```
Intensity(params) = (request latency) x (number of requests)
                    / sum((number of requests), (request latency)
```

The emissions per request includes the amortized training and fine-tuning emissions:
```
Em(inference, params) = (amortized training emissions) + (amortized fine-tuning emissions) +
                        (total emissions) x Intensity(params)

H2O(inference, params) = (amortized training water) + (amortized fine-tuning water) +
                        (total water) x Intensity(params)
```

## Various research findings

To model inference requires understanding the emissions per inference for a model on a particular host given certain parameters. There are *many* factors that determine how a model is executed including [batching strategies](https://www.anyscale.com/blog/continuous-batching-llm-inference#continuous-batching), [paged attention](https://blog.vllm.ai/2023/06/20/vllm.html), and optimizations like quantization. The intention of this methodology is to provide a framework that both calculates and predicts the emissions cost of inference with the flexibility to include future optimizations as they appear.

Inference speed may be limited by the framework or by compute. Maximum performance will be achieved when models are not limited by [framework overhead](https://arxiv.org/pdf/2302.06117).  In an optimal scenario, inference can use the full capacity and power of a GPU, as described by [Towards Pareto Optimal Throughput in Small Language Model Serving](https://arxiv.org/pdf/2404.03353). 
