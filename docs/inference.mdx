---
title: "Inference"
---


## Modeling Azure ND A100 v4-series GPU

From [Andrew A. Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. Reducing the Carbon Impact of Generative AI Inference (today and in 2035). In 2nd Workshop on Sustainable Computer Systems (HotCarbon â€™23), July 9, 2023, Boston, MA, USA. ACM, New York, NY, USA, 7 pages.](https://dl.acm.org/doi/pdf/10.1145/3604930.3605705):

ğ‘‡ğ·ğ‘ƒ = 0.428 kW per GPU (1/8 of 3.43 kW for the instance) x 1.1 PUE

ğ‘‚ğ¼ = 0.35 is TFLOPS per inference assuming GPT-3 model (around 175 billion weights) processed with BF16 operations. 

ğ¼ğ‘Š = 5 is the number of inferences per output word (assumed window/sampling of 5 for each output word)

ğ‘Šğ¶ is the output word count (measured average of 185 output words/request)

ğ¶ = 156 TFLOPS is the GPU capacity assuming 50% efficiency

ğ¸hğ‘¤ is per-GPU emission calculated as 1/8 of estimated per-instance emissions:
ğ¸hğ‘¤ = 1/8 (ğ‘ƒğ¹ +ğ¸ğºğ‘ƒğ‘ˆ +ğ¸ğ¶ğ‘ƒğ‘ˆ +ğ¸ğ·ğ‘…ğ´ğ‘€ +ğ¸ğ‘†ğ‘†ğ· +ğ¸ğ»ğ·ğ·)
where ğ‘ƒğ¹ is IC packaging Carbon footprint while ğ¸ğºğ‘ƒğ‘ˆ , ğ¸ğ¶ğ‘ƒğ‘ˆ , ğ¸ğ·ğ‘…ğ´ğ‘€, ğ¸ğ‘†ğ‘†ğ·, and ğ¸ğ»ğ·ğ· are GPU, CPU, memory, and storage emissions, respectively. We estimate these emissions based on previous reports [26] and instance hardware specifications [1, 3, 11], yielding ğ¸hğ‘¤ = 318 kgCO2 per GPU

### Water Use
44.8 L/8" wafer-layer (2015 data)
